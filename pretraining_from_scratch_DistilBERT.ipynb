{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cxTlAqIIyGCK",
        "AjRkNAk0yDoq"
      ],
      "authorship_tag": "ABX9TyMs2EDSNuLu/lLnF41G/lI7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanujsriv/_transformer_models_/blob/main/pretraining_from_scratch_DistilBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmTj1NuqrKgq"
      },
      "outputs": [],
      "source": [
        "# !pip install torch transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DistilBertTokenizer"
      ],
      "metadata": {
        "id": "cxTlAqIIyGCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "from transformers import DistilBertConfig, DistilBertForMaskedLM\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "import os,json\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "4CE5q66HrUw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os,json\n",
        "dir = '/home/student_no_backup/sakumar/summarization/MeanSum/datasets/yelp_dataset/'\n",
        "os.chdir(dir)"
      ],
      "metadata": {
        "id": "JQoXuTLNt4F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = []\n",
        "\n",
        "print('Reading review.json.....')\n",
        "with open(\"review.json\",'r') as f:\n",
        "    content = f.readlines()\n",
        "    for c in content:\n",
        "        text_data.append(json.loads(c)['text'])\n",
        "\n",
        "max_length = 150\n",
        "read = 100000\n",
        "data = text_data[:read]\n",
        "\n",
        "print('Read'+str(read)+' review.json.....DONE')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G556xvBEsHOD",
        "outputId": "f1b465cc-ca83-481a-a693-2f2b4c9f3f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading review.json.....\n",
            "Read100000 review.json.....DONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)"
      ],
      "metadata": {
        "id": "bTXXSjO7zRby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "config = DistilBertConfig(vocab_size=tokenizer.vocab_size)\n",
        "model = DistilBertForMaskedLM(config=config)\n",
        "encodings = tokenizer(data, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "train_dataset = TextDataset(encodings)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "vYhuk4z1tgoZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c06ce908-8038-4a19-9369-ebb1d41fe9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/student_no_backup/sakumar/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    progress_bar = tqdm(train_dataloader, desc=\"Epoch {:1d}\".format(epoch+1))\n",
        "    for batch in progress_bar:\n",
        "        inputs = {k: v.to(model.device) for k, v in batch.items()}\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.set_postfix({'loss': loss.item()})"
      ],
      "metadata": {
        "id": "TIsjf0EGrcS9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "e44c59a6-6d70-478b-c193-13e492fb445f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   0%|                                                                                           | 0/3125 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'backward'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'backward'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertConfig, DistilBertForMaskedLM, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define the DistilBERT configuration\n",
        "config = DistilBertConfig(\n",
        "    vocab_size=len(tokenizer.vocab),\n",
        "    max_position_embeddings=512,\n",
        "    n_heads=6,\n",
        "    n_layers=6,\n",
        "    dim=768,\n",
        "    hidden_dim=3072,\n",
        "    dropout=0.1,\n",
        "    attention_dropout=0.1,\n",
        "    activation=\"gelu\"\n",
        ")\n",
        "\n",
        "# Initialize the model\n",
        "model = DistilBertForMaskedLM(config=config)\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./distilbert-wikitext\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "# Create a Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the final model\n",
        "trainer.save_model(\"./distilbert-wikitext-final\")"
      ],
      "metadata": {
        "id": "pbIfCf3LthDT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "e641d038-f4a0-4583-e9a4-de08161def70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'load_dataset' from 'datasets' (unknown location)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistilBertConfig, DistilBertForMaskedLM, Trainer, TrainingArguments\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load and preprocess the dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikitext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikitext-103-raw-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_dataset' from 'datasets' (unknown location)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nzhXxEhKRID",
        "outputId": "4315cff9-b921-40ef-b612-5f0c31a3dbcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.17.1 dill-0.3.8 multiprocess-0.70.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets"
      ],
      "metadata": {
        "id": "00RhAMIbKU0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BertWordPieceTokenizer"
      ],
      "metadata": {
        "id": "AjRkNAk0yDoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "30RxFeK_0j8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = []\n",
        "print('Reading review.json.....')\n",
        "with open(\"review.json\",'r') as f:\n",
        "    content = f.readlines()\n",
        "    for c in content:\n",
        "        text_data.append(json.loads(c)['text'])\n",
        "\n",
        "max_length = 150\n",
        "read = 100000\n",
        "data = text_data[:read]\n",
        "\n",
        "print('Read'+str(read)+' review.json.....DONE')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHWO9sSJyLg6",
        "outputId": "5c2fa0d9-c68d-4d43-9ccb-6f30222ffecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading review.json.....\n",
            "Read100000 review.json.....DONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# paths = [str(x) for x in Path('data/original').glob('**/*.txt')]\n",
        "\n",
        "# tokenizer = BertWordPieceTokenizer(\n",
        "#         clean_text=True,\n",
        "#         handle_chinese_chars=False,\n",
        "#         strip_accents=False,\n",
        "#         lowercase=True\n",
        "# )\n",
        "# tokenizer.train(files=paths[:10], vocab_size=30_000, min_frequency=2,\n",
        "#                     limit_alphabet=1000, wordpieces_prefix='##',\n",
        "#                     special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])"
      ],
      "metadata": {
        "id": "11ezvolVx7Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "# Assuming `encodings` is your tokenized and encoded text data\n",
        "train_dataset = TextDataset(encodings)"
      ],
      "metadata": {
        "id": "ZI119R1kymsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokens = tokenizer('Hello, how are you?')\n",
        "# print(tokens)\n",
        "# tokenizer.decode(tokens['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vjf5KNIx9OK",
        "outputId": "f7cb33a0-8348-4665-bb3b-30921845c8cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 7592, 1010, 2129, 2024, 2017, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] hello, how are you? [SEP]'"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = Dataset(paths = [str(x) for x in Path('data/original').glob('**/*.txt')][50:70], tokenizer=tokenizer)\n",
        "# loader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
        "\n",
        "# test_dataset = Dataset(paths = [str(x) for x in Path('data/original').glob('**/*.txt')][10:12], tokenizer=tokenizer)\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4)"
      ],
      "metadata": {
        "id": "5LHtQVW0x-ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import DistilBertForMaskedLM, DistilBertConfig\n",
        "\n",
        "# config = DistilBertConfig(\n",
        "#     vocab_size=30000,\n",
        "#     max_position_embeddings=514\n",
        "# )\n",
        "\n",
        "# model = DistilBertForMaskedLM(config)"
      ],
      "metadata": {
        "id": "gIYr7yqwx_uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "beta1 = 0.99\n",
        "beta2 = 0.999\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "loader = torch.utils.data.DataLoader(train_dataset, batch_size=8)\n",
        "optim = optim.Adam(model.parameters(), learning_rate, betas=(beta1, beta2))"
      ],
      "metadata": {
        "id": "8cGtMvSSwsTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import DistilBertConfig, DistilBertForMaskedLM, AdamW\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "# dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='train')\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def encode(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "dataset = dataset.map(encode, batched=True)\n",
        "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
      ],
      "metadata": {
        "id": "LmN10nwc2DsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a PyTorch DataLoader\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "dataloader = DataLoader(dataset, batch_size=8, collate_fn=data_collator)\n",
        "\n",
        "# Initialize DistilBERT model\n",
        "config = DistilBertConfig(vocab_size=tokenizer.vocab_size, n_heads=12, n_layers=6, dim=768)\n",
        "model = DistilBertForMaskedLM(config=config)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Set up the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}')\n"
      ],
      "metadata": {
        "id": "sRJXdrNE2hh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# epochs = 100\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     loop = tqdm(loader, leave=True)\n",
        "\n",
        "#     # set model to training mode\n",
        "#     model.train()\n",
        "#     losses = []\n",
        "\n",
        "#     # iterate over dataset\n",
        "#     for batch in loop:\n",
        "#         optim.zero_grad()\n",
        "\n",
        "#         # copy input to device\n",
        "#         input_ids = batch['input_ids'].to(device)\n",
        "#         attention_mask = batch['attention_mask'].to(device)\n",
        "#         labels = batch['labels'].to(device)\n",
        "\n",
        "#         # predict\n",
        "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "#         # update weights\n",
        "#         loss = outputs.loss\n",
        "#         loss.backward()\n",
        "\n",
        "#         optim.step()\n",
        "\n",
        "#         # output current loss\n",
        "#         loop.set_description(f'Epoch {epoch}')\n",
        "#         loop.set_postfix(loss=loss.item())\n",
        "#         losses.append(loss.item())\n",
        "\n",
        "#     print(\"Mean Training Loss\", np.mean(losses))\n",
        "#     losses = []\n",
        "#     loop = tqdm(test_loader, leave=True)\n",
        "\n",
        "#     # set model to evaluation mode\n",
        "#     model.eval()\n",
        "\n",
        "#     # iterate over dataset\n",
        "#     for batch in loop:\n",
        "#         # copy input to device\n",
        "#         input_ids = batch['input_ids'].to(device)\n",
        "#         attention_mask = batch['attention_mask'].to(device)\n",
        "#         labels = batch['labels'].to(device)\n",
        "\n",
        "#         # predict\n",
        "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "#         # update weights\n",
        "#         loss = outputs.loss\n",
        "\n",
        "#         # output current loss\n",
        "#         loop.set_description(f'Epoch {epoch}')\n",
        "#         loop.set_postfix(loss=loss.item())\n",
        "#         losses.append(loss.item())\n",
        "#     print(\"Mean Test Loss\", np.mean(losses))"
      ],
      "metadata": {
        "id": "rfiiJ-T5yB8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch.utils.data import DataLoader, Dataset\n",
        "# from transformers import DistilBertConfig, DistilBertForMaskedLM, AdamW\n",
        "# from transformers import DataCollatorForLanguageModeling\n",
        "# # from datasets import load_dataset\n",
        "\n",
        "# # Load and preprocess the dataset\n",
        "# # dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='train')\n",
        "\n",
        "# dataset = train_dataset\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# def encode(examples):\n",
        "#     return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "# dataset = dataset.map(encode, batched=True)\n",
        "# dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "# # Create a PyTorch DataLoader\n",
        "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "# dataloader = DataLoader(dataset, batch_size=8, collate_fn=data_collator)\n",
        "\n",
        "# # Initialize DistilBERT model\n",
        "# config = DistilBertConfig(vocab_size=tokenizer.vocab_size, n_heads=12, n_layers=6, dim=768)\n",
        "# model = DistilBertForMaskedLM(config=config)\n",
        "\n",
        "# # Move model to GPU if available\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model.to(device)\n",
        "\n",
        "# # Set up the optimizer\n",
        "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# # Training loop\n",
        "# num_epochs = 3\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in dataloader:\n",
        "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
        "#         outputs = model(**batch)\n",
        "#         loss = outputs.loss\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}')"
      ],
      "metadata": {
        "id": "ZnBOFADXzf0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DistilBERT pretraining"
      ],
      "metadata": {
        "id": "6n2SklmA8q1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "from transformers import DistilBertConfig, DistilBertForMaskedLM\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "import os,json\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSwK1oie_XWE",
        "outputId": "3203264b-fbf4-415e-a650-52198886d56e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-02-28 14:37:01.248437: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-28 14:37:02.621314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/home/student_no_backup/sakumar/summarization/MeanSum/datasets/yelp_dataset/'\n",
        "os.chdir(dir)"
      ],
      "metadata": {
        "id": "XAjzxBwP_V9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = []\n",
        "print('Reading review.json.....')\n",
        "with open(\"review.json\",'r') as f:\n",
        "    content = f.readlines()\n",
        "    for c in content:\n",
        "        text_data.append(json.loads(c)['text'])\n",
        "\n",
        "max_length = 150\n",
        "read = 100000\n",
        "data = text_data[:read]\n",
        "\n",
        "print('Read'+str(read)+' review.json.....DONE')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1Woz-F__IQ7",
        "outputId": "ad12c3bd-530c-4485-8d4e-7e33efc43989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading review.json.....\n",
            "Read100000 review.json.....DONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k95ouvQI8tmv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}